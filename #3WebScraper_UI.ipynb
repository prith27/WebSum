{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb48f9cf-c585-46e5-aa64-10dc00a4456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba799ec6-0d7b-454e-868a-66244eab8019",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "170de10d-4427-47d7-9d72-921196a8dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'gpt-4o-mini'\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba43381b-7483-430d-9eca-6034515e35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "###Change here is that I add links as a variable to be accessed later, use for loop i.e get all links in the webpage then add all links into a list\n",
    "class Website:\n",
    "    \"\"\"\n",
    "    A utility class to represent a Website that we have scraped, now with links\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        if soup.body:\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            self.text = \"\"\n",
    "        links = [link.get('href') for link in soup.find_all('a')]\n",
    "        self.links = [link for link in links if link]\n",
    "\n",
    "    def get_contents(self):\n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a018780-733e-4ec5-b48c-abdde4ed1304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mailto:prithvi.seshadri01@gmail.com',\n",
       " 'tel:+919940654174',\n",
       " 'https://www.linkedin.com/in/prithvi-seshadri-b736631b3/',\n",
       " 'https://scholar.google.com/citations?user=aP4C_hoAAAAJ&hl=en',\n",
       " '#',\n",
       " 'https://ieeexplore.ieee.org/document/10395841',\n",
       " 'https://onlinelibrary.wiley.com/doi/10.1002/9781119905172.ch6',\n",
       " 'https://ai.plainenglish.io/a-beginners-guide-to-training-a-yolov5-object-detection-model-91adffe99f79',\n",
       " 'https://ai.plainenglish.io/building-accurate-object-detection-models-with-retinanet-a-comprehensive-step-by-step-guide-b8a35f435285',\n",
       " 'https://iopscience.iop.org/article/10.1088/1742-6596/2115/1/012038']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = Website(\"https://prith27.github.io/\")\n",
    "ps.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "192e586d-658f-4fd6-9b7d-69bfc4c680b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_system_prompt = \"You are provided with a list of links found on a webpage. \\\n",
    "You are able to decide which of the links would be most relevant to include in a brochure about the company, \\\n",
    "such as links to an About page, or a Company page, or Careers/Jobs pages.\\n\"\n",
    "link_system_prompt += \"You should respond in JSON as in this example:\"\n",
    "link_system_prompt += \"\"\"\n",
    "{\n",
    "    \"links\": [\n",
    "        {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
    "        {\"type\": \"careers page\": \"url\": \"https://another.full.url/careers\"}\n",
    "    ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d640775-cffe-466c-8f47-2e1e32cbda51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are provided with a list of links found on a webpage. You are able to decide which of the links would be most relevant to include in a brochure about the company, such as links to an About page, or a Company page, or Careers/Jobs pages.\n",
      "You should respond in JSON as in this example:\n",
      "{\n",
      "    \"links\": [\n",
      "        {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
      "        {\"type\": \"careers page\": \"url\": \"https://another.full.url/careers\"}\n",
      "    ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(link_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f131c0e2-9298-490f-b9f3-89c7423547ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_user_prompt(website):\n",
    "    user_prompt = f\"Here is the list of links on the website of {website.url} - \"\n",
    "    user_prompt += \"please decide which of these are relevant web links for a brochure about the company, respond with the full https URL in JSON format. \\\n",
    "Do not include Terms of Service, Privacy, email links.\\n\"\n",
    "    user_prompt += \"Links (some might be relative links):\\n\"\n",
    "    user_prompt += \"\\n\".join(website.links)\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f8c84e-a88d-465f-acd7-8cac8b3d39f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the list of links on the website of https://prith27.github.io/ - please decide which of these are relevant web links for a brochure about the company, respond with the full https URL in JSON format. Do not include Terms of Service, Privacy, email links.\n",
      "Links (some might be relative links):\n",
      "mailto:prithvi.seshadri01@gmail.com\n",
      "tel:+919940654174\n",
      "https://www.linkedin.com/in/prithvi-seshadri-b736631b3/\n",
      "https://scholar.google.com/citations?user=aP4C_hoAAAAJ&hl=en\n",
      "#\n",
      "https://ieeexplore.ieee.org/document/10395841\n",
      "https://onlinelibrary.wiley.com/doi/10.1002/9781119905172.ch6\n",
      "https://ai.plainenglish.io/a-beginners-guide-to-training-a-yolov5-object-detection-model-91adffe99f79\n",
      "https://ai.plainenglish.io/building-accurate-object-detection-models-with-retinanet-a-comprehensive-step-by-step-guide-b8a35f435285\n",
      "https://iopscience.iop.org/article/10.1088/1742-6596/2115/1/012038\n"
     ]
    }
   ],
   "source": [
    "print(get_links_user_prompt(ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cb3d8fa-12de-4b24-aa9e-e3c197eed390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "    website = Website(url)\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": link_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_links_user_prompt(website)}\n",
    "      ],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    return json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e1290c0-eb0e-4b6a-9cf3-c81f2cafdfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'links': [{'type': 'LinkedIn profile',\n",
       "   'url': 'https://www.linkedin.com/in/prithvi-seshadri-b736631b3/'},\n",
       "  {'type': 'Google Scholar profile',\n",
       "   'url': 'https://scholar.google.com/citations?user=aP4C_hoAAAAJ&hl=en'},\n",
       "  {'type': 'IEEE document',\n",
       "   'url': 'https://ieeexplore.ieee.org/document/10395841'},\n",
       "  {'type': 'Wiley publication',\n",
       "   'url': 'https://onlinelibrary.wiley.com/doi/10.1002/9781119905172.ch6'},\n",
       "  {'type': 'AI guide article',\n",
       "   'url': 'https://ai.plainenglish.io/a-beginners-guide-to-training-a-yolov5-object-detection-model-91adffe99f79'},\n",
       "  {'type': 'RetinaNet guide article',\n",
       "   'url': 'https://ai.plainenglish.io/building-accurate-object-detection-models-with-retinanet-a-comprehensive-step-by-step-guide-b8a35f435285'},\n",
       "  {'type': 'IOPscience article',\n",
       "   'url': 'https://iopscience.iop.org/article/10.1088/1742-6596/2115/1/012038'}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_links(\"https://prith27.github.io/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c098d8d-ed29-46cb-96b4-cf54acdf7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_details(url):  ###Here i am appending all website url details and info one below other, will understand if you see print statement\n",
    "    result = \"Landing page:\\n\"\n",
    "    result += Website(url).get_contents()\n",
    "    links = get_links(url)\n",
    "    print(\"Found links:\", links)\n",
    "    for link in links[\"links\"]:\n",
    "        result += f\"\\n\\n{link['type']}\\n\"\n",
    "        result += Website(link[\"url\"]).get_contents()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4c9ea20-1574-44d6-9efc-f8d713012a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found links: {'links': [{'type': 'linkedin page', 'url': 'https://www.linkedin.com/in/prithvi-seshadri-b736631b3/'}, {'type': 'scholar profile', 'url': 'https://scholar.google.com/citations?user=aP4C_hoAAAAJ&hl=en'}, {'type': 'IEEE document', 'url': 'https://ieeexplore.ieee.org/document/10395841'}, {'type': 'Wiley article', 'url': 'https://onlinelibrary.wiley.com/doi/10.1002/9781119905172.ch6'}, {'type': 'AI blog post 1', 'url': 'https://ai.plainenglish.io/a-beginners-guide-to-training-a-yolov5-object-detection-model-91adffe99f79'}, {'type': 'AI blog post 2', 'url': 'https://ai.plainenglish.io/building-accurate-object-detection-models-with-retinanet-a-comprehensive-step-by-step-guide-b8a35f435285'}, {'type': 'IOPscience article', 'url': 'https://iopscience.iop.org/article/10.1088/1742-6596/2115/1/012038'}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Landing page:\\nWebpage Title:\\nPrithvi Seshadri\\nWebpage Contents:\\nPrithvi Seshadri\\nData Scientist\\nShow Contacts\\nEmail\\nprithvi.seshadri01@gmail.com\\nPhone\\n+91 9940654174\\nBirthday\\nSept 27, 2001\\nLocation\\nChennai, Tamil Nadu, India\\nAbout\\nResume\\nBlog\\nContact\\nAbout me\\nHey there! I\\'m a Data Scientist at Shell, born in the BBQ haven of Houston, Texas, but currently rocking it in Chennai, India. By day, I wrangle data and deploy cutting-edge ML & DL algorithms to solve Shell’s asset-based mysteries with end-to-end CI/CD pipelines, saving the company millions.\\nWhen I’m not crunching numbers, you\\'ll find me on the football field. I\\'ve played Division 1 football and represented my school in national competitions, winning numerous local tournaments along the way.\\nOn the nerdy side, I love diving into data science research and have proudly published three papers in the field. Footballer by day, data scientist by... well, also by day. Welcome to my world!\\nWhat I do\\nData Science\\nLeveraging data to uncover patterns and developing advanced machine learning and deep learning solutions.\\nArtificial Intelligence\\nAI enthusiast with research experience in AI algorithms and large language models (LLMs).\\nMLOps\\nProfessional development of MLOps CI/CD pipelines utilizing Azure and GitHub Actions.\\nFootball\\nPassionate football enthusiast and Manchester United supporter, with experience playing Division 1 football in Chennai.\\nResume\\nEducation\\nVellore Institute of Technology\\n2019 — 2023\\nGraduated with a BTech in Computer Science, specializing in AI and ML, with a CGPA of 9.16/10. Conducted research in AI and ML, publishing three research papers. Played for the college football team, winning several inter-college tournaments.\\nDAV Public School\\n2017 — 2019\\nSecured 96.2% (481/500) in the CBSE AISSCE examinations, ranking third in the computer science department. Led the school football team as captain, representing the school in national-level tournaments and winning multiple local football tournaments.\\nExperience\\nData Scientist @ Shell\\nAug 2023 — Present\\nDesigned and implemented fully automated CI/CD pipelines on Azure, specializing in deploying advanced Machine Learning and Deep Learning algorithms. Tackled complex challenges unique to Shell\\'s assets, achieving approximately $9 million in savings through predictive maintenance models.\\nData Analyst Intern @ Acies Global\\nMay 2022 — Jun 2022\\nAnalyzed global market trends to forecast sales and profit fluctuations. Collaborated with clients to rectify missing sales data across various regions. Utilized Amperity and Snowflake for efficient data extraction and analysis.\\nMy skills\\nMachine Learning and Deep Learning\\n90%\\nMLOps\\n90%\\nData Modeling and Analysis\\n90%\\nArtificial Intelligence\\n85%\\nPublications & Blogs\\nComputer Vision\\nNov 18, 2023\\nHand Detection and Morse Code Translation for Alternative Communication\\nIEEE International Conference on System, Computation, Automation and Networking (ICSCAN)\\nML & DL\\nJun 2, 2022\\nDetection of Phishing URLs Using Machine Learning and Deep Learning Models Implementing a URL Feature Extractor\\nEvolution and Applications of Quantum Computing, Scrivener Publishing LLC\\nComputer Vision\\nFeb 24, 2023\\nA Beginner’s Guide to Training a YOLOv5 Object Detection Model\\nMedium.com\\nComputer Vision\\nFeb 20, 2023\\nBuilding Accurate Object Detection Models with RetinaNet: A Comprehensive Step-by-Step Guide\\nMedium.com\\nAI & DL\\nNov 24, 2021\\nWeb Based COVID Detection System using Deep Learning\\nJournal of Physics: Conference Series, IOP Science\\nContact\\nContact Form\\nSend Message\\n\\n\\n\\nlinkedin page\\nWebpage Title:\\nNo title found\\nWebpage Contents:\\n\\n\\n\\n\\nscholar profile\\nWebpage Title:\\n\\u202aPrithvi Seshadri\\u202c - \\u202aGoogle Scholar\\u202c\\nWebpage Contents:\\nLoading...\\nThe system can\\'t perform the operation now. Try again later.\\nCitations per year\\nDuplicate citations\\nThe following articles are merged in Scholar. Their\\ncombined citations\\nare counted only for the first article.\\nMerged citations\\nThis \"Cited by\" count includes citations to the following articles in Scholar. The ones marked\\n*\\nmay be different from the article in the profile.\\nAdd co-authors\\nCo-authors\\nFollow\\nNew articles by this author\\nNew citations to this author\\nNew articles related to this author\\'s research\\nEmail address for updates\\nDone\\nMy profile\\nMy library\\nMetrics\\nAlerts\\nSettings\\nSign in\\nSign in\\nGet my own profile\\nCited by\\nAll\\nSince 2020\\nCitations\\n3\\n3\\nh-index\\n1\\n1\\ni10-index\\n0\\n0\\n0\\n2\\n1\\n2023\\n2024\\n1\\n2\\nFollow\\nPrithvi Seshadri\\nData Engineer,\\nShell\\nVerified email at shell.com\\nData Science\\nArtificial Intelligence\\nComputer Vision\\nArticles\\nCited by\\nTitle\\nSort\\nSort by citations\\nSort by year\\nSort by title\\nCited by\\nCited by\\nYear\\nDetection of Phishing URLs Using Machine Learning and Deep Learning Models Implementing a URL Feature Extractor\\nA Mahesh, P Seshadri, S Mishra, SK Satapathy\\nEvolution and Applications of Quantum Computing, 93-110\\n, 2023\\n2\\n2023\\nHand Detection and Morse Code Translation for Alternative Communication\\nP Seshadri, S Dananjayan, HFT Ahmed\\n2023 International Conference on System, Computation, Automation and\\xa0…\\n, 2023\\n1\\n2023\\nWeb Based COVID Detection System using Deep Learning\\nS Srivarshan, P Seshadri, E Kaarthikand, A Vijayalakshmi\\nJournal of Physics: Conference Series 2115 (1), 012038\\n, 2021\\n2021\\nConversational Hate-Offensive detection in Code-Mixed Hindi-English Tweets\\nR Rajalakshmi, S Srivarshan, F Mattins, E Kaarthik, P Seshadri\\nCEUR Workshop Proceedings, 1-11\\n, 2021\\n2021\\nThe system can\\'t perform the operation now. Try again later.\\nArticles 1–4\\nShow more\\nPrivacy\\nTerms\\nHelp\\nAbout Scholar\\nSearch help\\n\\n\\n\\nIEEE document\\nWebpage Title:\\nHand Detection and Morse Code Translation for Alternative Communication | IEEE Conference Publication | IEEE Xplore\\nWebpage Contents:\\nIEEE Account\\nChange Username/Password\\nUpdate Address\\nPurchase Details\\nPayment Options\\nOrder History\\nView Purchased Documents\\nProfile Information\\nCommunications Preferences\\nProfession and Education\\nTechnical Interests\\nNeed Help?\\nUS & Canada:\\n+1 800 678 4333\\nWorldwide:\\n+1 732 981 0060\\nContact & Support\\nAbout IEEE\\nXplore\\nContact Us\\nHelp\\nAccessibility\\nTerms of Use\\nNondiscrimination Policy\\nSitemap\\nPrivacy & Opting Out of Cookies\\nA not-for-profit organization, IEEE is the world\\'s largest technical professional organization dedicated to advancing technology for the benefit of humanity.\\n© Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.\\n\\n\\n\\nWiley article\\nWebpage Title:\\nJust a moment...\\nWebpage Contents:\\nEnable JavaScript and cookies to continue\\n\\n\\n\\nAI blog post 1\\nWebpage Title:\\nA Beginner’s Guide to Training a YOLOv5 Object Detection Model | by Prithvi Seshadri | Artificial Intelligence in Plain English\\nWebpage Contents:\\nOpen in app\\nSign up\\nSign in\\nWrite\\nSign up\\nSign in\\nA Beginner’s Guide to Training a YOLOv5 Object Detection Model\\nPrithvi Seshadri\\n·\\nFollow\\nPublished in\\nArtificial Intelligence in Plain English\\n·\\n12 min read\\n·\\nFeb 24, 2023\\n--\\n1\\nListen\\nShare\\nObject detection is a fascinating field that has gained a lot of attention in recent years due to its wide range of applications in areas such as self-driving cars, security systems, and healthcare. YOLOv5 (You Only Look Once version 5) is a state-of-the-art object detection model that has become increasingly popular due to its high accuracy and fast processing speed. In this beginner’s guide, we will explore the steps involved in training a YOLOv5 model from scratch, including data preparation, model configuration, and training. Whether you’re a beginner in deep learning or an experienced practitioner, this guide will provide you with a solid foundation to train your own YOLOv5 model and explore the exciting field of object detection.\\nIn order to train a YOLOv5 object detection model, you must have a dataset of annotated images that indicates the location of the objects you want the model to detect. While open-source datasets like COCO and Pascal VOC are available, a custom dataset can be more precise for your specific use case. Dividing the dataset into training and validation sets is also a crucial step to ensure the accuracy and reliability of your model.\\nThis article aims to guide you through the entire process of training and evaluating your own YOLOv5 model. We will provide an introduction to the capabilities of YOLOv5 and discuss how to annotate and prepare your dataset for training. We will then delve into the important aspect of model training, providing you with step-by-step instructions to train your YOLOv5 model from scratch. Lastly, we will explain how to evaluate your model’s performance, giving you all the necessary knowledge to get started with object detection using YOLOv5. Whether you are new to object detection or looking to improve your skills, this comprehensive guide will equip you with the tools needed to train a YOLOv5 object detection model with confidence.\\nIntroduction\\nYOLOv5 is the latest and greatest version of the YOLO (You Only Look Once) family of real-time object detection models. Developed by Glenn Jocher and the team at Ultralytics, YOLOv5 represents a complete overhaul from its predecessors, YOLOv4 and YOLOv3, and boasts significant improvements in both speed and accuracy. In fact, YOLOv5 has achieved state-of-the-art results in object detection across a range of datasets, including the well-known COCO and Pascal VOC benchmarks.\\nSo what makes YOLOv5 so special? Its architecture! YOLOv5 leverages a novel backbone network called CSPDarknet, which uses cross-stage partial connections to improve information flow and feature reuse. Plus, it integrates cutting-edge techniques like Spatial Attention Modules and Swish activation functions to enhance object detection accuracy even further. All this while maintaining real-time inference speeds of up to 140 frames per second on a single GPU, making it one of the fastest and most accurate object detection models out there. If you’re looking to develop applications in fields like autonomous vehicles, robotics, or surveillance systems, YOLOv5 is definitely worth a closer look.\\nDataset and Annotation Tool\\nThe foundation of any successful object detection project is a well-curated dataset. The dataset provides the model with the necessary information to recognize and localize objects accurately. Therefore, before diving into model training, it is crucial to gather or create a good dataset that covers a wide range of scenarios and variations that you anticipate the model to encounter in the real world.\\nCustom datasets play a critical role in achieving high accuracy and specificity in object detection tasks. While several open-source datasets are available, such as the COCO dataset and EgoHands dataset, creating a custom dataset that caters to your specific use case can be more effective. Annotation tools such as\\nLabelImg\\nand\\nRoboflow\\ncan help in creating such a dataset. In my previous article titled\\n“Building Accurate Object Detection Models with RetinaNet: A Comprehensive Step-by-Step Guide”\\n,\\nI utilized LabelImg for image annotation. However, in this article, I will be demonstrating how to use Roboflow, a powerful annotation tool that can streamline the dataset preparation process.\\nRoboflow is an all-in-one data management platform designed for computer vision tasks, such as object detection, image classification, and segmentation. It offers a range of tools for image annotation, dataset generation, and model deployment. With its integrations with popular deep learning frameworks like TensorFlow and PyTorch, Roboflow makes it easy to train models using your custom dataset.\\nOne of the standout features of Roboflow is its intuitive web-based image annotation tool. The tool allows you to label images with bounding boxes, polygons, and other shapes that indicate the location of objects in the image. Its user-friendly interface and efficiency make it easy to label large datasets quickly and accurately. Additionally, Roboflow offers various features for managing datasets, including data augmentation, data cleaning, and data export. It also provides pre-processing options like resizing and normalization, which can improve model performance.\\nOverall, Roboflow is an excellent tool for preparing and managing custom datasets for computer vision tasks. Its comprehensive features and user-friendly interface make it ideal for both beginners and experienced practitioners. To start using Roboflow, create an account and set up a new workspace to begin creating new projects.\\nAfter creating a new project in Roboflow, you can begin uploading the images you want to annotate. The process is simple and easy to follow. You can upload images directly from your computer or from cloud storage services such as Google Drive or Dropbox. Once the images are uploaded, you can begin annotating them using the platform’s powerful annotation tools. An example of the upload process can be seen in the image below:\\nPhoto Upload in Roboflow\\nAfter uploading your images to Roboflow, the next step is to assign them for annotation. Annotation is a crucial process in creating a custom dataset, and Roboflow makes it easy with its intuitive interface. Simply draw bounding boxes around the objects of interest, and the tool will automatically save the annotations. Once you have annotated your images, a dialog box will appear, allowing you to split the images into training, validation, and test sets. It is recommended to have at least 60–70% of your data as training data, as this provides the model with enough examples to learn from. However, the split can be adjusted to meet your specific needs. With Roboflow’s annotation tool and easy-to-use data management features, creating a custom dataset for object detection tasks has never been easier.\\nTo demonstrate the process of uploading images for annotation, I have randomly selected 8 images. These images are used as examples to illustrate the annotation process in Roboflow.\\nImage Annotation\\nAfter completing the annotation process, you can create a dataset by dividing the annotated images into train, validation, and test sets:\\nAdding Images to the dataset\\nAfter adding images to the dataset and annotating them, the next step is to generate a new version of the dataset for exporting to your local IDE or Google Colab. During the generation process, you can choose the preprocessing and augmentation steps as per your requirements. For instance, you can resize the images or apply data augmentation techniques like rotation or flipping to increase the diversity of the dataset. Once the generation process is complete, you can easily export your dataset and use it for training your object detection model.\\nData Preprocessing Step\\nData Augmentation Step\\nAfter generating a new version of the dataset, the next step is to export it in a suitable format for training the model. Roboflow offers the flexibility to export the dataset in various formats, including YOLOv5 Pytorch format, which we will use in this example. This format is widely used for training object detection models and is compatible with popular deep learning frameworks such as PyTorch. Once you have selected the desired export format, Roboflow will generate a download link for the exported dataset. You can then use this link to download the dataset to your local IDE or Google Colab for further processing and training.\\nChoosing Data Export Format\\nIt’s crucial to save the download link that is generated after exporting your dataset in a text file for easy access, especially if you plan on using it in a Google Colab notebook. Additionally, an API key is generated along with the download link, which should be kept in a secure location to prevent unauthorized access. Make sure not to disclose this key to anyone.\\nExport Dataset\\nNow that we have prepared our dataset, it’s time to dive into the exciting part of the object detection process: training the model. This step is where the algorithm learns to identify and classify objects in images, allowing it to make accurate predictions on unseen data. So let’s get started!\\nModel Training\\nIn this article, we will be using Colab to train the YOLOv5 object detection model on our custom dataset. Google Colab is a powerful and user-friendly platform for training deep learning models. The first step to getting started with YOLOv5 on Colab is to clone the YOLOv5 GitHub repository. This repository contains all the necessary code and configuration files required for training the model. Cloning the repository can be done by running a simple command in the Colab notebook:\\n!git clone https://github.com/ultralytics/yolov5\\nAfter cloning the YOLOv5 GitHub repository, we need to set up the environment and configure the model for training. This involves several steps such as installing the required dependencies, configuring the paths for the dataset and the model configuration files, and specifying the training parameters such as batch size, learning rate, and number of epochs.\\nTo install the required dependencies, we need to switch into the YOLOv5 directory using the\\n%cd yolov5\\ncommand in the Colab notebook. To confirm that we are in the correct directory, we can\\nimport os\\nmodule and run the command\\nprint(os.getcwd())\\nto print the current working directory. This ensures that we are in the right place to run the training script for our YOLOv5 model.\\n%cd yolov5\\nimport os\\nprint(os.getcwd())\\nOnce you are in the right directory, we can run\\n%pip install -qr requirements.txt\\nto install the dependencies listed in the requirements.txt file. We can also install Roboflow, which is required to access our annotated dataset, using\\n%pip install -q roboflow\\n. These commands will set up the necessary environment for training our YOLOv5 model.\\n%cd yolov5\\n%pip install -qr requirements.txt\\n%pip install -q roboflow\\nAfter setting up the environment, the next step is to import Roboflow and access the dataset that was created in Roboflow. To do this, you can copy the code that was generated and saved in a text file during the dataset export process into your Colab notebook. Once you have pasted the code, you can run it to download the dataset and load it into your Colab environment. This will allow you to start training your YOLOv5 model using the annotated data.\\n!pip install roboflow\\nfrom roboflow import Roboflow\\nrf = Roboflow(api_key=\"add-your-api-key\")\\nproject = rf.workspace(\"vit-bf5j3\").project(\"hand-detection-kyj5o\")\\ndataset = project.version(1).download(\"yolov5\")\\nTo start training, we can use the following command:\\n!python train.py --img 640 --batch 8 --epochs 100 --data {dataset.location}/data.yaml --weights yolov5s.pt --cache\\nIn this command, we are specifying the following parameters:\\n--img\\n: The size of the input images during training. Here, we are setting it to 640x640 pixels.\\n--batch\\n: The batch size for training. Here, we are setting it to 8.\\n--epochs\\n: The number of epochs to train for. Here, we are setting it to 100.\\n--data\\n: The location of the dataset YAML file which we created in Roboflow.\\n--weights\\n: The location of the pre-trained YOLOv5 model weights. Here, we are using the\\nyolov5s.pt\\nfile.\\n--cache\\n: Whether or not to cache images for faster training.\\nOnce we run this command, the training process will begin and we can monitor the progress in the Colab notebook. During the training process, YOLOv5 saves two types of checkpoint files:\\nlast.pt\\nand\\nbest.pt\\n.\\nlast.pt\\nis the latest saved checkpoint of the model. It is updated after each training epoch and contains the weights of the model at that point in the training process. This checkpoint can be used to resume training from where it was left off or to evaluate the model\\'s performance at a certain epoch.\\nbest.pt\\nis the checkpoint that has the best validation loss so far. It is updated whenever the validation loss improves and contains the weights of the model at that point. This checkpoint is useful for model selection as it represents the point in the training process where the model performed the best on the validation set. Both\\nlast.pt\\nand\\nbest.pt\\ncan be used for inference after the training process is complete.\\nThe\\n.pt\\nfile can be downloaded from Google Colab to your local machine using the following command in your Colab notebook:\\nfrom google.colab import files\\nfiles.download(\\'/content/yolov5/runs/train/exp/weights/best.pt\\')\\nThis command will download the\\nbest.pt\\nfile, which contains the weights of the best-performing model during training. It is important to save this file in a secure location, as it represents the trained model and can be used to make predictions on new images or to continue training the model later.\\nIt is not necessary to convert a trained YOLOv5 model in\\n.pt\\nformat to\\n.h5\\nformat, as the\\n.pt\\nformat is already compatible with PyTorch and can be used for inference or further training. However, if you wish to convert the model to\\n.h5\\nformat, you can use the following steps:\\nInstall the tensorflow and tensorflow_addons libraries in your Python environment.\\n!pip install tensorflow tensorflow_addons\\n2.\\nLoad the .pt model using PyTorch and export it in ONNX format.\\nimport torch\\n# Load the trained model from the .pt file\\nmodel = torch.hub.load(\\'ultralytics/yolov5\\', \\'custom\\', path_or_model=\\'path/to/trained.pt\\')\\n# Export the model to ONNX format\\ntorch.onnx.export(model, torch.randn(1, 3, 640, 640), \\'yolov5.onnx\\', opset_version=11)\\n3.\\nConvert the ONNX model to .h5 format using the TensorFlow converter.\\nimport tensorflow as tf\\nimport tensorflow_addons as tfa\\n# Load the ONNX model\\nmodel = onnx.load(\\'yolov5.onnx\\')\\n# Convert the model to TensorFlow format\\ntf_rep = onnx_tf.backend.prepare(model)\\n# Convert the TensorFlow model to .h5 format\\ntf.keras.models.save_model(tf_rep, \\'yolov5.h5\\', save_format=\\'h5\\')\\nNote that the conversion from PyTorch to ONNX format may result in some loss of precision, and the conversion from ONNX to TensorFlow format may result in some differences in behavior due to differences in the implementations of certain operations. Therefore, it is generally recommended to use the .pt format directly if possible.\\nModel Evaluation\\nTo evaluate the performance of the trained model, you can use TensorBoard, which is a powerful tool for visualizing and analyzing machine learning experiments. To use TensorBoard in Colab, you can load the TensorBoard extension using the\\n%load_ext tensorboard\\ncommand, and then run\\n%tensorboard --logdir runs\\nto start the TensorBoard server. This will launch a TensorBoard dashboard in a new tab, where you can view various metrics and visualizations related to the training process, such as loss curves, accuracy, precision, and recall. You can also compare the performance of different models and experiments using TensorBoard.\\n%load_ext tensorboard\\n%tensorboard --logdir runs\\nTensorboard Evaluations\\nIn conclusion, building an object detection model using YOLOv5 and Roboflow can be a relatively simple and efficient process. With the help of Roboflow, the time-consuming task of annotating images can be greatly reduced, allowing for more time to focus on training and fine-tuning the model. Additionally, Google Colab provides a convenient and accessible platform for training and evaluating the model. By following the steps outlined in this article, you can build an object detection model that can accurately detect objects in images and videos, and even deploy it for use in real-world applications.\\nThe YOLOv5 model offers a great balance between accuracy and speed, making it a popular choice for object detection tasks. However, it’s important to note that creating a high-quality dataset and tweaking the training parameters can significantly impact the model’s accuracy. With that in mind, get started with building your own object detection model today and see the exciting results!\\nMore content at\\nPlainEnglish.io\\n.\\nSign up for our\\nfree weekly newsletter\\n. Follow us on\\nTwitter\\n,\\nLinkedIn\\n,\\nYouTube\\n, and\\nDiscord\\n.\\nBuild awareness and adoption for your tech startup with\\nCircuit\\n.\\nYolov5\\nDeep Learning\\nMachine Learning\\nArtificial Intelligence\\nTransfer Learning\\n--\\n--\\n1\\nFollow\\nPublished in\\nArtificial Intelligence in Plain English\\n16.2K Followers\\n·\\nLast published\\n15 hours ago\\nNew AI, ML and Data Science articles every day. Follow to join our 3.5M+ monthly readers.\\nFollow\\nFollow\\nWritten by\\nPrithvi Seshadri\\n4 Followers\\n·\\n3 Following\\nFollow\\nResponses (\\n1\\n)\\nSee all responses\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams\\n\\n\\n\\nAI blog post 2\\nWebpage Title:\\nBuilding Accurate Object Detection Models with RetinaNet: A Comprehensive Step-by-Step Guide | by Prithvi Seshadri | Artificial Intelligence in Plain English\\nWebpage Contents:\\nOpen in app\\nSign up\\nSign in\\nWrite\\nSign up\\nSign in\\nBuilding Accurate Object Detection Models with RetinaNet: A Comprehensive Step-by-Step Guide\\nPrithvi Seshadri\\n·\\nFollow\\nPublished in\\nArtificial Intelligence in Plain English\\n·\\n12 min read\\n·\\nFeb 20, 2023\\n--\\nListen\\nShare\\nThis article aims to provide a comprehensive guide on how to train a state-of-the-art object detection model called RetinaNet. Object detection is a fundamental computer vision task that involves identifying and localizing objects in an image or a video. RetinaNet is a popular object detection model that has shown impressive results on various benchmark datasets, thanks to its unique architecture that balances the trade-off between localization and classification accuracy.\\nIf you’re an AI enthusiast looking to learn how to build a RetinaNet model from scratch, you’re in the right place! This article takes the reader through the entire process of building a RetinaNet model, starting from preparing the training data to training and testing a RetinaNet model. By the end, you’ll have a solid understanding of the entire process and be ready to create your own object detection model. The article will cover the following topics in detail:\\nAn introduction to RetinaNet and its architecture\\nData preparation and annotation\\nModel training and evaluation\\nBy the end of this article, the reader will have a clear understanding of how to build an end-to-end object detection pipeline with RetinaNet and will be able to apply this knowledge to solve real-world computer vision problems.\\nIntroduction\\nRetinaNet is a state-of-the-art object detection model that was introduced in 2017 by Facebook AI Research. It is a single, unified network architecture that can be used for both object detection and object classification, making it very efficient and accurate. RetinaNet uses a feature pyramid network (FPN) to extract features at different scales and a novel focal loss function that is designed to give more weight to complex examples, resulting in better performance on small and hard-to-see objects. RetinaNet has achieved impressive results on several benchmark datasets and is widely used for various computer vision tasks.\\nDataset and Annotation Tool\\nTo train an object detection model, the first step is to gather a dataset of images and annotate them with labels that identify the objects of interest. This annotation process can be accomplished with the help of an annotation tool such as\\nLabelImg\\n, which allows you to draw bounding boxes around the objects in each image and label them with a corresponding class name.\\nLabelImg is an open-source graphical image annotation tool used to label object bounding boxes in images. It provides an easy-to-use interface for annotating images with object detection labels. LabelImg supports various formats such as Pascal VOC, YOLO, and Tensorflow. The tool is written in Python and Qt, and is available on multiple platforms, including Windows, Linux, and macOS. It is widely used by researchers and practitioners for creating datasets for training object detection models. Once you have annotated your dataset, you can use it to train your object detection model using a framework such as RetinaNet.\\nLabelImg can be easily installed with just one command! All you need to do is open up your terminal and type\\npip3 install labelImg\\n. With this simple command, you\\'ll have the annotation tool up and running in no time, ready to help you annotate your dataset and prepare it for use in training your deep learning models. Once the installation is complete, you can use the\\nlabelImg\\ncommand in your terminal to start the tool and begin annotating your images. If you encounter any difficulties during the installation process, don’t worry! Detailed installation steps for each operating system are available on the official GitHub page for LabelImg. Simply head over to\\nhttps://github.com/heartexlabs/labelImg\\nand you’ll find everything you need to get up and running with this powerful annotation tool.\\nAnnotating images using LabelImg is a breeze, and the tool even generates .xml files automatically as soon as you’re done annotating an image. This saves you time and effort and ensures that your annotations are consistently formatted and ready to be used in your deep-learning models. With its intuitive interface and user-friendly features, LabelImg is the perfect tool for annotating your dataset quickly and accurately.\\nModel Training\\nTo keep your dataset organized, it’s recommended to create a parent folder for your RetinaNet project. Inside this folder, you should create two child folders named “JPEGImages” and “Annotations”. The “JPEGImages” folder should contain all the original image files in the .jpg format, while the “Annotations” folder should contain all the .xml files generated using LabelImg. This will help you keep track of your files and ensure a smooth training process. Here’s an example of what your file structure could look like:\\nRetinaNet File Structure\\nBy organizing your files in this way, you’ll be able to easily load your dataset into the RetinaNet model during training. If you’re using Google Colab for training your RetinaNet model, the next step is to upload the parent folder that you created into your Google Drive account. Once the folder is uploaded, you can access it directly from your Colab notebook. In the following steps, I will be explaining how to train your RetinaNet model using Google Colab.\\nTo connect your Google Drive with Colab, you can use the following code snippet:\\nfrom google.colab import drive\\ndrive.mount(\\'/content/drive\\')\\nThis will prompt you to authenticate your account and provide an authorization code. Once you have provided the code, your Google Drive will be mounted in Colab, and you can access your files and folders by navigating to the\\n/content/drive\\ndirectory. To start working with RetinaNet in Colab, the first step is to clone the\\nkeras-retinanet\\nrepository. You can easily do this by running the following command in a code cell:\\n!git clone https://github.com/fizyr/keras-retinanet.git\\nThis will clone the entire repository into your current working directory, allowing you to access all the necessary files for training and testing RetinaNet. Once this is done,\\nimport os\\ncheck the current working directory using the\\nos.getcwd()\\ncommand. Switch to the k\\neras-retinanet\\ndirectory using the\\n%cd keras-retinanet/\\ncommand. Once you are in this directory, you need to perform a few installations. The installation commands are given below:\\n!pip install .\\n!python setup.py build_ext --inplace\\nNow that the installation is complete, it’s time to import the necessary libraries. Some of the key libraries that you’ll need to include are given below:\\nimport numpy as np\\nimport pandas as pd\\nimport os, sys, random\\nimport xml.etree.ElementTree as ET\\nfrom keras_retinanet.utils.visualization import draw_box, draw_caption , label_color\\nfrom keras_retinanet.utils.image import preprocess_image, resize_image\\nimport shutil\\nfrom os.path import isfile, join\\nimport matplotlib.pyplot as plt\\nfrom PIL import Image\\nimport requests\\nimport urllib\\nfrom os import listdir\\nTo make things easier, you can store the paths of the JPEG images and corresponding XML files in variables. This way, you can easily access these files during training. Here’s an example of how you can do this:\\njpgPath=\"/content/drive/MyDrive/RetinaNet/JPEGImages/\"\\nannPath=\"/content/drive/MyDrive/RetinaNet/Annotations/\"\\nTo train the RetinaNet model using the\\nKeras-RetinaNet repository\\n, you need to create a dataframe with attributes that the repository supports. The following code snippet can be used to create the dataframe:\\ndata=pd.DataFrame(columns=[\\'fileName\\',\\'xmin\\',\\'ymin\\',\\'xmax\\',\\'ymax\\',\\'class\\'])\\nTo start the annotation process, you can read in all the annotation files and extract data from these files into the previously created dataframe. This can be achieved by using the\\nos\\nlibrary to iterate through the annotations directory and read in each file using the\\nElementTree\\nlibrary. Once the data has been extracted, it can be stored in the dataframe created earlier. This process is critical in preparing the dataset for training your RetinaNet model, and must be done accurately. It can be done in the following way:\\nall_files = [f for f in listdir(annPath) if isfile(join(annPath, f))]\\nfor file in all_files:\\nif file.split(\".\")[-1] != \\'xml\\':\\ncontinue\\nfilename = jpgPath + file.replace(\".xml\", \".jpg\")\\ntree = ET.parse(join(annPath, file))\\nroot = tree.getroot()\\nfor obj in root.iter(\\'object\\'):\\nclass_name = obj.find(\\'name\\').text\\nxml_box = obj.find(\\'bndbox\\')\\nxmin = int(xml_box.find(\\'xmin\\').text)\\nymin = int(xml_box.find(\\'ymin\\').text)\\nxmax = int(xml_box.find(\\'xmax\\').text)\\nymax = int(xml_box.find(\\'ymax\\').text)\\ndata = data.append({\\'filename\\': filename, \\'xmin\\': xmin, \\'ymin\\': ymin, \\'xmax\\': xmax, \\'ymax\\': ymax, \\'class\\': class_name}, ignore_index=True)\\nThe above code snippet loops through all the XML annotation files in the annotation folder, extracts the class labels, bounding box coordinates and the corresponding file name, and then appends them to the dataframe. The ‘if’ condition checks if the file extension is ‘xml’. The file name of the corresponding image is obtained by replacing the ‘.xml’ extension with ‘.jpg’. The bounding box coordinates and class labels are then extracted using ElementTree (ET) parsing. Finally, the extracted data is appended to the dataframe. This allows for an easy and efficient way of gathering all the necessary information for training the RetinaNet model, as the dataframe can easily be converted to a CSV file, which is used for training the model.\\nAfter the dataframe is created, it can be viewed using the\\ndata.head()\\ncommand. In order to use this data for training the RetinaNet model, you need to convert it into a CSV file. This can be done by using the following command to remove the indices and headers:\\ndata.to_csv(\\'../TrainData.csv\\',header=False,index=False)\\nTo ensure that all model snapshots are saved in a specific location after each epoch, you can create a “snapshots” folder using the following steps. First, check if the folder already exists or not. If it doesn’t exist, create it using the\\nos.mkdir()\\ncommand.\\nif not os.path.exists(\\'snapshots\\'):\\nos.mkdir(\\'snapshots\\')\\nTo get a list of unique class names from the\\ndata\\ndataframe, you can use the following code:\\nclasses=data[\\'class\\'].unique()\\n. This will give you an array of all the unique class names present in the\\n\\'class\\'\\ncolumn of the\\ndata\\ndataframe. To create a new file and write the class names and indices, you can use the built-in Python function\\nopen()\\nwith the file mode set to\\n\\'w\\'\\n(write mode). This will create a new file if it doesn\\'t exist or overwrite the existing file if it does.\\nwith open(\\'../Classes.csv\\', \\'w\\') as file:\\nfor i, class_name in enumerate(classes):\\nfile.write(f\\'{class_name},{i}\\\\n\\')\\nIn the above code, the\\nopen()\\nfunction is used to create a new file called\\n\\'../handDetectorClasses.csv\\'\\nwith the file mode set to\\n\\'w\\'\\n. The file is opened in write mode which allows data to be written to it. The\\n\\'w\\'\\nmode overwrites the file if it already exists.\\nThe for loop iterates over each unique class name in the\\nclasses\\nlist and writes the class name and its corresponding index to the file separated by a comma. The\\n\\\\n\\ncharacter at the end of each line is used to indicate a new line in the file. This file will be used to map class names to class indices for the RetinaNet model during the training process.\\nIt is recommended to start with a pre-trained model rather than training a model from scratch. In our case, we will use the ResNet50 model, which is already pre-trained on the Coco dataset. This can be done using the code snippet given below:\\nurl = \\'https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5\\'\\nmodel = \\'/content/keras-retinanet/snapshots/resnet50_csv_v1.h5\\'\\nurllib.request.urlretrieve(url, model)\\nThe above code utilizes the\\nurllib.request.urlretrieve()\\nfunction to download a ResNet model specified by a URL and save it to a desired path. The URL of the model and the path to save the model are stored in the variables\\nurl\\nand\\nmodel\\nrespectively.\\nThe\\nurllib.request.urlretrieve()\\nfunction takes two arguments, the URL of the file to be downloaded and the local file path where the file should be saved. When called, the function downloads the file and saves it to the specified file path. In this case, the ResNet model will be downloaded from the specified URL and saved to the path specified by the\\nmodel\\nvariable.\\nTo train the\\nkeras-retinanet\\nmodel in Colab, follow the steps below:\\n!keras_retinanet/bin/train.py\\n--freeze-backbone\\n--random-transform\\n--weights \\'/content/keras-retinanet/snapshots/resnet50_csv_v1.h5\\'\\n--batch-size 8\\n--steps 500\\n--epochs 15 csv \\'path/to/TrainData.csv\\' \\'path/to/classes.csv\\'\\nThe above command is used to train a RetinaNet object detection model on the specified training data using a pre-trained ResNet50 backbone. The arguments can be explained as follows:\\n--freeze-backbone\\n: This argument freezes the ResNet50 backbone of the RetinaNet model, which is already trained on the COCO dataset.\\n--random-transform\\n: This argument applies random transformations on the images during training to augment the data and reduce overfitting.\\n--weights\\n: This specifies the path to the pre-trained ResNet50 model we downloaded earlier, which will be used as the backbone of the RetinaNet model.\\n--batch-size\\n: This specifies the batch size used for training.\\n--steps\\n: This specifies the number of steps (batches) per epoch.\\n--epochs\\n: This specifies the number of epochs to train the model.\\ncsv \\'path/to/TrainData.csv\\' \\'path/to/classes.csv\\'\\n: This specifies the paths to the CSV files containing the training data and class information.\\nTo load the trained model, we can use the\\nglob\\nmodule to find the path of the saved model in the snapshots folder. Here\\'s an example:\\nfrom glob import glob\\nmodel_path = glob.glob(\\'/content/keras-retinanet/snapshots/*.h5\\')[-1]\\nfrom keras_retinanet import models\\nmodel = models.load_model(model_path, backbone_name=\\'resnet50\\')\\nIn the above code, we use the\\nglob\\nfunction to find all files in the\\nsnapshots\\nfolder with the extension\\n.h5\\n. The\\n[-1]\\nat the end of the line returns the last file found, which is assumed to be the most recent model saved during training. We then load the model using the\\nload_model\\nfunction from\\nkeras_retinanet.models\\n, passing in the path of the saved model and the name of the backbone used during training (in this case,\\nresnet50\\n). The loaded model can then be used to make predictions on new data. Now we can start making predictions!\\nLet\\'s define a simple function to make predictions:\\ndef show_predictions(filename, threshold=0.5):\\n# Construct the path to the image file\\nfile_path = os.path.join(jpgPath, filename)\\nprint(f\\'File path: {file_path}\\')\\n# Load the image and its annotations\\nimage_df = data[data[\\'fileName\\'] == file_path]\\nimage = np.array(Image.open(file_path))[:, :, :3]  # Remove alpha channel if any\\n# Draw the ground-truth bounding boxes on the image\\nfor _, row in image_df.iterrows():\\nbox = [row[\\'xmin\\'], row[\\'ymin\\'], row[\\'xmax\\'], row[\\'ymax\\']]\\ndraw_box(image, box, color=(255, 0, 0))\\n# Preprocess the image and make predictions with the model\\ninput_image = preprocess_image(image)\\ninput_image, scale = resize_image(input_image)\\nboxes, scores, labels = model.predict_on_batch(np.expand_dims(input_image, axis=0))\\nboxes /= scale\\n# Draw the predicted bounding boxes on the image, along with their labels and scores\\nfor box, score, label in zip(boxes[0], scores[0], labels[0]):\\nif score < threshold:\\nbreak\\nbox = box.astype(np.int32)\\ncolor = label_color(label)\\ndraw_box(image, box, color=color)\\nclass_name = label_map[label]\\ncaption = f\"{class_name} {score:.3f}\"\\ndraw_caption(image, box, caption)\\nscore, label = score, label\\n# Display the final image with the annotations\\nplt.figure(figsize=(20, 10))\\nplt.imshow(image)\\nplt.axis(\\'off\\')\\nplt.show()\\nreturn score, label\\nThe above function,\\nshow_predictions()\\n, takes an image filename from the dataset and generates object detection predictions for that image. The function first reads the image from the specified file path and extracts the bounding boxes of the objects in the image from the dataset. It then preprocesses the image, resizes it, and passes it through the trained RetinaNet model to generate predictions for the objects in the image. Finally, the function visualizes the image with the predicted bounding boxes and their corresponding class labels and scores. This function can be used to quickly evaluate the performance of the trained model on individual images in the dataset.\\nTo generate predictions for a given image in the dataset, you can use the\\nshow_predictions\\nfunction by passing the name of the image to the function. For instance, to retrieve predictions for an image called\\nimage_name.jpg\\nin the\\njpgPath\\ndirectory, you can use the following code:\\nscore, label = show_predictions(\\'image_name.jpg\\', threshold=0.5)\\nThe\\nthreshold\\nargument specifies the confidence threshold for the predicted objects. The function first loads the image and gets its shape, then iterates over the rows in the\\ndata\\ndataframe to draw bounding boxes for the ground truth objects in the image. It then preprocesses the image and passes it to the trained model for object detection, and draws the predicted bounding boxes for the objects with a score above the specified threshold.\\nFinally, the function displays the image with the predicted bounding boxes and their corresponding labels. The\\nscore\\nand\\nlabel\\nvariables contain the scores and labels for the predicted objects, respectively.\\nBefore we wrap up, it’s important to keep in mind that while object detection can be a powerful tool, it is not without limitations. Like any machine learning model, object detection algorithms have their own set of biases and limitations that can affect their accuracy and effectiveness. For example, if the model is trained on a dataset that is not diverse enough, it may struggle to recognize objects that are not well-represented in the training data. Similarly, object detection can be used in ways that are unfair or unjust, such as for surveillance or discriminatory profiling.\\nAs with any technology, it’s important to approach object detection with a critical eye and to carefully consider how it can be used in a way that is ethical and fair. By being aware of these limitations and biases, we can work to ensure that object detection is used in a way that benefits society as a whole.\\nWhile object detection has come a long way, it’s important to remember that we’re not quite in a “Terminator” world just yet. Despite its limitations, object detection technology holds incredible potential to make our lives easier and safer. Whether you’re using it to track down lost pets or to detect potential hazards in industrial settings, there’s no denying that object detection is an exciting and constantly evolving field. So let’s keep training those models, fine-tuning those hyperparameters, and building a future where our machines can truly see the world around us!\\nMore content at\\nPlainEnglish.io\\n.\\nSign up for our\\nfree weekly newsletter\\n. Follow us on\\nTwitter\\n,\\nLinkedIn\\n,\\nYouTube\\n, and\\nDiscord\\n.\\nBuild awareness and adoption for your tech startup with\\nCircuit\\n.\\nRetinanet\\nDeep Learning\\nMachine Learning\\nArtificial Intelligence\\nTransfer Learning\\n--\\n--\\nFollow\\nPublished in\\nArtificial Intelligence in Plain English\\n16.2K Followers\\n·\\nLast published\\n15 hours ago\\nNew AI, ML and Data Science articles every day. Follow to join our 3.5M+ monthly readers.\\nFollow\\nFollow\\nWritten by\\nPrithvi Seshadri\\n4 Followers\\n·\\n3 Following\\nFollow\\nNo responses yet\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams\\n\\n\\n\\nIOPscience article\\nWebpage Title:\\nRadware Captcha Page\\nWebpage Contents:\\nWe apologize for the inconvenience...\\n...but your activity and behavior on this site made us think that you are a bot.\\nNote:\\nA number of things could be going on here.\\nIf you are attempting to access this site using an anonymous Private/Proxy network, please disable that and try accessing site again.\\nDue to previously detected malicious behavior which originated from the network you\\'re using, please request unblock to site.\\nIncident ID:\\n76295719-cnvj-48f3-934b-0d1da833d30a\\nPlease solve this CAPTCHA to request unblock to the website\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_details(\"https://prith27.github.io/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ced1409-8aac-4f8c-bb5c-343e023ad9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of several relevant pages from a company website \\\n",
    "and creates a short brochure about the company for prospective customers, investors and recruits. Respond in markdown.\\\n",
    "Include details of company culture, customers and careers/jobs if you have the information.\"\n",
    "\n",
    "def get_brochure_user_prompt(company_name, url):\n",
    "    user_prompt = f\"You are looking at a company called: {company_name}\\n\"\n",
    "    user_prompt += f\"Here are the contents of its landing page and other relevant pages; use this information to build a short brochure of the company in markdown.\\n\"\n",
    "    user_prompt += get_all_details(url)\n",
    "    user_prompt = user_prompt[:5_000] # Truncate if more than 5,000 characters\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "814a0f14-7c99-4462-9283-74ed90934fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_brochure(company_name, url):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_brochure_user_prompt(company_name, url)}\n",
    "          ],\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c80fc506-8186-41cc-b5a4-6c971996826c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found links: {'links': []}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Prithvi Seshadri - Data Scientist at Shell\n",
       "\n",
       "---\n",
       "\n",
       "## About Me\n",
       "Hello! I’m Prithvi Seshadri, a Data Scientist currently working with Shell in Chennai, India. Originally from the BBQ haven of Houston, Texas, I’ve transformed my love for analyzing complex data into actionable insights that have saved millions for my company.\n",
       "\n",
       "When not wrangling data or developing advanced machine learning algorithms, I indulge my passion for football, having played Division 1 and represented my school at national competitions.\n",
       "\n",
       "---\n",
       "\n",
       "## What I Do\n",
       "### Data Science\n",
       "Utilizing data to uncover patterns and developing advanced machine learning and deep learning solutions tailored for asset management.\n",
       "\n",
       "### Artificial Intelligence\n",
       "Research enthusiast with hands-on experience in AI algorithms and large language models.\n",
       "\n",
       "### MLOps\n",
       "Specialist in deploying fully automated CI/CD pipelines with expertise in Azure and GitHub Actions.\n",
       "\n",
       "### Football\n",
       "A dedicated Manchester United supporter and passionate football player.\n",
       "\n",
       "---\n",
       "\n",
       "## Education\n",
       "- **Vellore Institute of Technology**  \n",
       "  BTech in Computer Science (AI and ML Specialization)  \n",
       "  Graduated: 2023 | CGPA: 9.16/10\n",
       "\n",
       "- **DAV Public School**  \n",
       "  AISSCE (CBSE) | 96.2% (Ranked 3rd in Computer Science)  \n",
       "  Captain of the school football team.\n",
       "\n",
       "---\n",
       "\n",
       "## Experience\n",
       "### Data Scientist @ Shell  \n",
       "*Aug 2023 — Present*  \n",
       "- Designed automated CI/CD pipelines on Azure for deploying advanced ML & DL algorithms.\n",
       "- Implemented predictive maintenance models achieving approximately $9 million in cost savings.\n",
       "\n",
       "### Data Analyst Intern @ Acies Global  \n",
       "*May 2022 — Jun 2022*  \n",
       "- Analyzed market trends to forecast sales fluctuations using Amperity and Snowflake.\n",
       "\n",
       "---\n",
       "\n",
       "## Publications & Blogs\n",
       "1. **Computer Vision**  \n",
       "   *\"Hand Detection and Morse Code Translation for Alternative Communication\"*  \n",
       "   IEEE International Conference on System, Computation, Automation and Networking (ICSCAN) - Nov 18, 2023\n",
       "\n",
       "2. **ML & DL**  \n",
       "   *\"Detection of Phishing URLs Using Machine Learning and Deep Learning Models\"*  \n",
       "   Evolution and Applications of Quantum Computing, Scrivener Publishing LLC - Jun 2, 2022\n",
       "\n",
       "3. **Computer Vision**  \n",
       "   *\"A Beginner's Guide to Training a YOLOv5 Object Detection Model\"* - Medium.com - Feb 24, 2023\n",
       "\n",
       "4. **Computer Vision**  \n",
       "   *\"Building Accurate Object Detection Models with RetinaNet: A Comprehensive Step-by-Step Guide\"* - Medium.com - Feb 20, 2023\n",
       "\n",
       "5. **AI & DL**  \n",
       "   *\"Web Based COVID Detection System using Deep Learning\"*  \n",
       "   Journal of Physics: Conference Series, IOP Science - Nov 24, 2021\n",
       "\n",
       "---\n",
       "\n",
       "## Connect with Me\n",
       "For collaboration or inquiries, feel free to reach out:\n",
       "- **Email**: [prithvi.seshadri01@gmail.com](mailto:prithvi.seshadri01@gmail.com)\n",
       "- **Phone**: +91 9940654174\n",
       "\n",
       "---\n",
       "\n",
       "Join me in making data-driven decisions and exploring the exciting intersection of technology and sports!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_brochure(\"Prithvi Website\", \"https://prith27.github.io/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3da6a61-4bc8-4a0a-b3cd-ee77c4100363",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Now UI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3561f8f-c2ef-4fe6-98ba-31006b0adf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of several relevant pages from a company website \\\n",
    "and creates a short brochure about the company for prospective customers, investors and recruits. Give the response in a structured manner which looks good.\\\n",
    "Include details of company culture, customers and careers/jobs if you have the information.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b13f3777-73af-4065-8c63-396687fdd9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    stream = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97eeac87-2b8f-401f-becf-139e716567d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_brochure(company_name, url):\n",
    "    prompt = get_brochure_user_prompt(company_name,url)\n",
    "    result = stream_gpt(prompt)\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "238a2956-595a-409a-94f0-80c09b88198c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found links: {'links': [{'type': 'about page', 'url': 'https://www.shell.com/about-us.html'}, {'type': 'careers page', 'url': 'https://www.shell.com/careers.html'}, {'type': 'company page', 'url': 'https://www.shell.com/company.html'}, {'type': 'sustainability page', 'url': 'https://www.shell.com/sustainability.html'}, {'type': 'news page', 'url': 'https://www.shell.com/media.html'}]}\n"
     ]
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_brochure,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Company name:\"),\n",
    "        gr.Textbox(label=\"Landing page URL including http:// or https://\")],\n",
    "    outputs=[gr.Markdown(label=\"Brochure:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0e983-aebb-4345-b2c0-5d746bd2cb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
